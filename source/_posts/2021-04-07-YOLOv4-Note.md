---
title: YOLOv4学习笔记
cover: https://cdn.jsdelivr.net/gh/qxdn/qxdn-assert@0.3.0/top-img.png
tags:
  - 机器学习
categories: 机器学习
date: 2021-04-07 15:24:54
description: 因为一直没有懂yolov4，因此这次重新阅读YOLOv4并且留下一点点笔记。其实基本是简要翻译(逃)，结果没简要到哪里去建议直接看总结
---


因为一直没有懂yolov4，因此这次重新阅读YOLOv4并且留下一点点笔记。其实基本是简要翻译(逃)，结果没简要到哪里去建议直接看[总结](#总结)
![YOLOv4和其他最先进的检测器对比](https://cdn.jsdelivr.net/gh/qxdn/qxdn-assert@0.3.0/Comparison-of-YOLOv4.png)
封面来源《9nine》
<!--more-->

# 主要工作
&emsp;&emsp;作者在这方面指出了，大多数基于CNN的检测器大量部署于推荐系统。提高实时对象检测器的准确度不仅可以用于推荐系统生成提示，还可以用于独立的过程管理和减少人工输入。传统图形处理单元(GPU)上的实时目标检测器操作允许以经济实惠的价格大规模使用。最准确的现代神经网络不能实时进行操作，且需要大量的GPU进行小批次的训练。
&emsp;&emsp;作者希望能够使得YOLOv4易于训练，同时可以使用传统的GPU来获得实时的高质量且令人信服的检测效果。
&emsp;&emsp;其工作总结如下：
1. 开发了一个高效且强大的物体检测模型。使得任何人都可以使用1080 Ti或者2080 Ti GPU来训练一个超快且准确的物体检测器。
2. 在训练检测器的时候，验证了最先进的物体检测中的Bag of Freebies和Bag of Specials方法(关于这两个词，可以见[Bag of freebies](#bag-of-freebies)和[Bag of speicals](#bag-of-specials))
3. 修改了最先进的方法，使其更高效，更适合单个GPU训练，包含了CBN,PAN,SAM等

# 相关工作
## 物体检测方法
&emsp;&emsp;在此处，作者对现代的一些检测方法进行了简单的介绍。现代的检测器通常由两部分组成。一个在ImageNet上经过预训练的骨架(backbone)和一个用于预测类和物体边界框的头(head)。对于那些运行在GPU平台上的检测器，其backbone可能是VGG、ResNet、ResNeXt或者DenseNet。对于那些运行在CPU平台上的检测器，其backbone可能是SqueezeNet，MobileNet或者ShuffleNet。对于head部分，其通常可以被分类为两种，即one-stage的物体检测器和two-stage的物体检测器。最具代表性的two-stage物体检测器是R-CNN系列，包含fast R-CNN，faster R-CNN,R-FCN和Libra R-CNN。也可以使得two-stage物体检测器变成一个anchor-free(无锚框)的物体检测器，比如RepPoints。对于one-stage检测器，最具代表性的就是YOLO,SSD和RetinaNet。最近几年也有anchor-free的one-stage检测器，比如CenterNet，CornerNet,FCOS等。近年来的物体检测发展中，经常在head和backbone之间插入一些层，这些层被称为检测器的颈部(neck)。通常neck由几个bottom-up路径和几个top-down路径组成。使用这种配置的网络包含Feature Pyramid Network(FPN),Path Aggregation Network(PAN)BiFPN和NAS-FPN。除了上述模型，一些研究者还专注于直接搭建一个新的backbone(DetNet,DetNAS)或者一个全新的物体检测模型(SpineNet，HitDetector)。
## 普通目标检测器的组成部分
- **Input:** Image,Patches,Image Pyramid
- **Backbone:** VGG16,ResNet,SpineNet,EfficientNet-B0/B7,CSPResNeXt50,CSPDarknet53
- **Neck:** 
    - **Additional block:** SPP,ASPP,RFB,SAM
    - **Path-aggregation block:**FPN,BiFPN,ASFF,SFAM
- **Heads:**
    - **Dense Prediction(one-stage):**
        - RPN,SSD,YOLO,RetinaNet(anchor based)
        - CornerNet,CenterNet,MatrixNet,FCOS(anchor free)
    - **Sparse Prediction(two-stage):**
        - Faster R-CNN,R-FCN,Mask R-CNN(anchor based)
        - RepPoints(anchor free)

## Bag-of-freebies
&emsp;&emsp;通常，一个常规的物体检测器是离线训练的。因此研究者经常利用这一优势，使用更好的训练方法，在不增加推理成本的情况下获得更好的准确率。我们将这种只改变了训练策略或者只增加了训练成本的方法称为”Bag of freebies“。物体检测常用的且符合以上定义的方法是数据增强。数据增强的目的是增加输入图像的多样性，从而使得模型在不同的环境下有着高鲁棒性。比如，photometric distoitions和geometric distortions是用来数据增强方法的两个常用的手段。在处理photometric distortion中，我们会调整图像的亮度，对比度，色调，饱和度以及噪声。对于geometric distortion，我们会随机增加尺度变化，裁剪，翻转以及旋转。
&emsp;&emsp;上述提到的数据增强都是像素级别的调整，他保留了调整区域的所有原始像素的信息。一些学者在数据增强方面专注于模拟物体被遮挡。他们在图像分类和物体检测上有好的效果。比如，random erase和CutOut可以随机选择矩形区域融合或者零填充。对于hide-and-seek和grid mask，它们随机或者均匀的选择多个矩形区域，并以零替换。如果将相似的概念用来特征图中，则有DropOut, DropConnect和DropBlock方法。此外，一些研究者还提出使用多张图片进行数据增强。比如，MixUp使用两张图片相乘并以不同的系数进行重叠，并调整标签。对于CutMix，它将裁剪的图片覆盖到其他图片的矩形区域，然后根据混合区域的大小调整标签。除了以上方法，style transfer GAN也可以用于数据增强。这种用法可以有效地减少CNN所学习的纹理偏差。
&emsp;&emsp;与上述提到的各种方法不同，其他一些bag of freebies用于解决数据集中的语义分布可能存在偏差的问题，处理语义分布存在偏差问题，一个非常重要的问题就是不同类别直接的数据不平衡，这个问题在two-stage物体检测器中通过困难负样本挖掘或者在线困难样本挖掘解决。但是样本挖掘不适合于one-stage检测器，因为这类检测器属于dense prediction architecture结构。因此Lin等人提出focal loss来解决不同类别数据不均衡问题，另外一个非常重要的问题就是one-hot编码很难描述不同类别之间的关联度关系。label smoothing提出在训练时将硬标签转换为软标签，使得模型更具有鲁棒性。为了获得更好的软标签，Islam等人提出了知识蒸馏的概念来设计标签细化网络。
&emsp;&emsp;最后一个bag of freebies是设计边界框回归的目标函数。传统物体检测器通常使用MSE对边框的中心坐标、高、宽$\{x_{center},y_{center},w,h\}$或者左上角和右下角两个坐标点$\{x_{top\_left},y_{top\_left},x_{bottom\_left},y_{bottom\_right}\}$。使用anchor-based方法，其调整为预测偏差offset$\{x_{center\_offset},y_{center\_offset},w_{offset},h_{offset}\}$和$\{x_{top\_left\_offset},y_{top\_right\_offset},x_{bottom\_left\_offset},y_{bottom\_right\_offset}\}$但是，直接估计bbox的每个点坐标值是将这些点作为独立的变量，没有考虑将目标物体当成一个整体进行预测。为了更好的解决这个问题，一些学者提出了IoU函数，将BBox区域和ground true的BBox区域作为整体。IoU loss需要计算BBox四个坐标点以及ground truth的IoU。因为IoU具有尺度不变性，它可以解决传统算法比如计算$\{x,y,w,h\}$$l_1$, $l_2$的损失函数时存在的问题，这个损失函数会随着尺度的变化而发生变化。一些学者继续提升IoU loss函数，比如，GIoU loss包含了目标物体的形状和坐标，他们提出找到同时包含预测区域BBox和ground true BBox的最小区域，然后用这个BBox替代IoU loss原来的分母。DIoU loss额外考虑了目标物体的中心距离，CIoU另一方面同时将覆盖区域，中心点距离和纵横比考虑在内。CIoU在BBox回归问题上可以获得最好的收敛速度和准确率。DIoU loss额外考虑了物体中心距离。另一方面，CIoU同时将覆盖区域，中心点距离和纵横比考虑在内。CIoU在BBox回归问题上可以获得最好的收敛速度和准确率。

## Bag-of-specials
&emsp;&emsp;对于那些稍微增加了推理成本，但是可以极大地提升目标检测的准确率的插件模块和后处理方法，我们称之为“bag of specials”。一般来说，这些插件模块用来提高一个模型中特定的属性，比如增加感受野，引入注意力机制或者提高特征整合的能力等，后处理方式是用来抑制模型预测结果的一种方法。
常用的提升感受野的方法是SPP，ASPP和RFB。SPP模型来源于空间金字塔匹配(SPM)，SPM原本的方法是将特征图划分成d×d个相等的块，其中d可以是{1,2,3,...}，因此可以形成空间金字塔，然后提取bag-of-word的特征。SPP将SPM应用在CNN中，然后使用max-pooling代替bag-of-word运算。因为SPP输出的是一维特征向量，所以其不能运用在全卷积网络(FCN)中。因此，在YOLOv3中，Redmon和Farhadi改进了SPP模块，将max-pooling的输出和内核为k×k连接，k为{1,5,9,13}，strike为1。
&emsp;&emsp;ASPP模块和改进SPP模块的区别主要在于原始的k×k的内核大小，max-pooling的最大步长为1到3×3内核，膨胀比为k，拓展卷积运算步长为1。RFB模块使用一些×k的内核，膨胀比为k，步长为1的拓展卷积，它比ASPP获得了更全面的空间覆盖率。（这段一直没懂）
物体检测上常用的attention模型主要分成channel-wise attention模块和point-wise attention模块，两个模块的代表性模型是Squeeze-and-Excitation(SE)和Spatial Attention Module(SAM)。SE模块虽然在ImageNet上仅仅增加了2%的计算量和1%的top-1准确率，但是在GPU上提高了10%的推理时间，因此SE更适合在移动设备运行。而对于SAM，在ImageNet分类任务中，它只需要0.1%的额外计算就能提升ResNet50-SE 0.5%的top-1的准确率，它在GPU上没有有效地影响推理速度。
&emsp;&emsp;关于特征融合，早期实验使用skip connection或者hyper-column来将低层物理特征和高层语义特征融合。因为多尺度预测方法比如FPN变的流行，因此许多将不同将特征金字塔融合的轻量级模型被提出。这些模型包含了SFAM，ASFF和BiFPN。SFAM的主要思想是在多尺度连接特征图上使用channel-wise级别的调整。对于ASFF，它使用softmax作为point-wise级别的调整，然后将不同尺度的特征图加在一起。在BiFPN中，提出使用多输入权重残差连接去执行scale-wise级别的调整，然后将不同尺度的特征图加在一起。
&emsp;&emsp;在最近的一些研究中，一些研究者将注意力放在了寻找优秀的激活函数。一个好的激活函数可以将梯度更有效的进行传播，同时不会增加额外的计算量。在2010年，Nair和Hinton提出了ReLU来解决梯度消失问题，这个问题在传统的tanh和sigmod激活函数中经常会遇到。随后，LReLu,PReLU,ReLU6,Scaled Exponential Linear Unit(SELU),Swish,hard-Swish和Mish等激活函数被提出，他们用于解决梯度消失问题。LReLU和PReLU主要用来解决当输出小于零的时候，ReLU的梯度为零的问题。ReLU6和hard-Swish主要为量化网络而设计。对于神经网络的自归一化，提出SELU激活函数去实现这个目的。需要注意的是Swish和Mish都是连续可导的激活函数。
&emsp;&emsp;基于深度学习的物体检测中常用的后处理方法是NMS，可以过滤掉效果不好的BBox，保留效果较好的BBox。NMS尝试改进的方法和优化目标方程的方法类似。NMS提出的最初的方法并没有将上下文信息考虑在内，因此Girshick等人在R-CNN中添加了分类置信度作为参考，然后根据置信度得分的顺序，由高到低执行greedy NMS，DIoU NMS的开发者将重心坐标的距离信息添加到Bbox的筛选处理中了。值得一提的是，上面提到的后处理方法中都不直接引用捕获的图像特征，后续的anchor-free方法开发中不再需要后处理。

# YOLOv4 网络结构、BoF、BoS选择
&emsp;&emsp;经过大量的实验验证，作者使用CSPDarknet53作为骨架，利用SPP提高其感受野，PANet作为Neck，YOLOv3作为head。对于训练激活函数，因为PReLU和SELU难以训练，并且ReLU6是专门为量化网络设计的，我们因此不考虑这这三个激活函数。正则化方面，选择了DropBlock，因为它比其他正则化方法效果好。因为只关注在一块GPU上训练，因此不考虑CGBN、SyncBN。
&emsp;&emsp;此外，作者还提出了以下额外的改进
- 新的数据增强方法：Mosaic和Self-Adversarial Training(SAT)
- 使用遗传算法选择最优超参数
- 改进现有算法，使其更适合YOLOv4高效训练和检测：改进了SAM、PAN和Cross mini-Batch Normalization(CmBN)

## Mosaic
&emsp;&emsp;Mosaic是一种新型的数据增广的算法，它混合了四张训练图片。因此有四种不同的上下文进行融合，然而CutMix仅仅将两张图片进行融合。此外，batch normalization在每个网络层中计算四张不同图片的激活统计。这极大减少了一个大的mini-batch尺寸的需求。
![mosaic](https://cdn.jsdelivr.net/gh/qxdn/qxdn-assert@0.3.0/mosaic.png)

## SAT
&emsp;&emsp;自适应对抗训练(SAT)也表示了一个新的数据增广的技巧，它在前后两阶段上进行操作。在第一阶段，神经网络代替原始的图片而非网络的权重。用这种方式，神经网络自己进行对抗训练，代替原始的图片去创建图片中此处没有期望物体的描述。在第二阶段，神经网络使用常规的方法进行训练，在修改之后的图片上进检测物体。

## CmBN
&emsp;&emsp;CmBN（Cross mini-Batch Normalization）代表CBN改进的版本。它只收集了一个批次中的mini-batches之间的统计数据。
![CmBN](https://cdn.jsdelivr.net/gh/qxdn/qxdn-assert@0.3.0/CmBN.png)

## SAM,PAN
&emsp;&emsp;我们将SAM的spatial-wise注意力变成了point-wise注意力机制，然后将PAN中的shortcut连接变成了concatenation连接，正如下图所表示的那样。
![SAM](https://cdn.jsdelivr.net/gh/qxdn/qxdn-assert@0.3.0/SAM.png)
![PAN](https://cdn.jsdelivr.net/gh/qxdn/qxdn-assert@0.3.0/PAN.png)


# YOLOv4细节
| 网络结构 | 采用模块 | BoF | BoS |
| :----: | :----: | :---: | :---: |
| Backbone | CSPDarknet53 | CutMix,Mosaic data augmentation,DropBlock regularization,Class label smoothing| Mish activation,Cross-stage partial connections(CSP),Multi-input weight residual connections(MiWRC) |
| Neck | SPP,PAN | | |
| Head | YOLOv3 |CIoU-loss,CmBN,DropBlock regulariztion,Mosaic data augementation,Self-Adversarial Training,Eliminate grid sensitivity,Using multiple anchors for a single ground truth, Cosine annealing scheduler, Optimal hyperparameters, Random training shapes |Mish activation, SPP-block, SAM-block, PAN path-aggregation block, DIoU-NMS|

# 总结
&emsp;&emsp;YOLOv4这一论文，先是讲述了一些现代目标检测算法的发展，讲述了backbone、Neck、Head的组成和作用，讲述了目标检测器的组成部分，见[普通目标检测器的组成部分](#普通目标检测器的组成部分)。
&emsp;&emsp;随后讲解了Bag of freebies，其基本只改变训练策略。而Bag of specials基本为模块和后处理方法，提升了检测准确度。BoF,BoS的总结可以见下。这里部分可以看作是目标检测领域的发展了，也可以了解到各种方法的优缺点和原理。在之后的内容中作者对各种结构进行了实验并挑选网络结构，还自行提出了一定的创新，如Mosaic数据增强,自适应对抗训练(SAT)，利用遗传算法选择最优超参数，改进了CmBN、SAM和PAN。最终的YOLOv4使用的内容可以见[YOLOv4细节](#YOLOv4细节)。
&emsp;&emsp;YOLOv4这篇内容讲了许多技术内容，如果能弄懂这些名词的话也相当于是弄懂了物体检测的入门知识了。
&emsp;&emsp;最后放一张YOLOv4结构图，出处[Windows版YOLOv4目标检测：原理与源码解析](https://www.pianshen.com/article/10371541073/)
![structure](/images/YOLOv4-Note/YOLOv4.jpg)
## bag of freebies
- **bag of freebies**
    - **数据增强**
        - **像素级别**
            - photometric distortions
            - geometric distortions
        - **物体遮挡**
            - random erase
            - CutOut
            - hide and seek
            - grid mask
            - DropOut
            - DropConnect
            - DropBlock
        - **图片融合**
            - MixUP
            - CutMix
            - style transfer GAN
    - **样本不均衡**
        - **two-stage**
            - hard negative example mining
            - online hard example mining
        - **one-stage**
            - focal loss
    - **类别关联度难以描述**
        - label smoothing
    - **边界框回归目标函数**
        - MSE
        - GIoU
        - DIoU
        - CIoU
## Bag of specials
- **bag of specials**
    - **plugins modules**
        - **提高感受野**
            - SPP
            - ASPP
            - RFB
        - **attention模块**
            - **channel-wise**
                - Squeeze-and-Excitation(SE)
            - **point-wise**
                - Spatial Attention Module(SAM)
        - **特征融合**
            - skip connection
            - hyper-column
            - FPN
            - SFAM
            - ASFF
            - BiFPN
        - **激活函数**
            - tanh
            - sigmod
            - ReLU
            - LReLU
            - PReLU
            - ReLU6
            - SELU
            - Swish
            - hard-Swish
            - Mish
    - **后处理方法**
        - NMS
        - greedy NMS
        - DIoU NMS

## 吐槽
&emsp;&emsp;后面的实验结果就不再翻译了。先吐糟一下自己，本来是想写写概要的结果变成了简要翻译，而且也没有简要到哪里去，尴尬。
# 参考
[[1]Bochkovskiy A, Wang C Y, Liao H Y M. Yolov4: Optimal speed and accuracy of object detection[J]. arXiv preprint arXiv:2004.10934, 2020.](https://arxiv.org/abs/2004.10934)
[YOLOv4原文翻译 - v4它终于来了！](https://blog.csdn.net/qq_38316300/article/details/105759305)
[Windows版YOLOv4目标检测：原理与源码解析](https://www.pianshen.com/article/10371541073/)